[hyperparameters]
layer_num=3
reg=1e-4
ssl_mode=both
description="Go MLGN"
#the batch size for bpr loss training procedure
batch_size=2048
#the embedding size of lightGCN
recdim=64
#the learning rate
lr=.001
#the weight decay for l2 normalizaton
weight_decay=1e-4
#using the dropout or not
dropout=0
#the batch size for bpr loss training procedure
keepprob=.6
#the fold num used to split large adj matrix, like gowalla
a_fold=100
#the batch size for bpr loss training procedure
testbatch=128
#available datasets: {lastfm, gowalla, yelp2018, amazon-book,movielens}
dataset=movielens
#path to save weights
path=./checkpoints
#@k test list
topks=[10]
num_epoch=1000
#whether we use multiprocessing or not in test
multicore=0
#whether we use pretrained weight or not
pretrain=0
#random
seed=2022
#alpha hyperparameter: [0,0.1,0.5,1.0]
alpha=0.5
#the path to movielens dataset
dataset_dir=E:/pycharmprojects/projects/data/movielens
#whether we load test data of movielens or not
is_load_test_dataset=1
#MoCo base encoder, support:{embedding, mlp_v1, mlp_v2}
default_encoder=embedding
#task for light and SSL, support:{normal, multitasks, pretrain}
train_type=normal
#verbose mode, show log in console
verbose=1
#epoch % [test_step] == 0: start testing
test_step=10
#weight initializator, support:{normal, xavier}
init=xavier
#optimization algorithms, support:{adam, adagrad}
optim=adam
#log to server
log_api=
#save models or not, default not
save_flag=True
# temperature
temp=0.2

#other
A_split=False
;adj_type=plain, norm, gcmc, pre
adj_type=pre