[hyperparameters]
layer_num=3
reg=1e-4
description=""
#the batch size for bpr loss training procedure
batch_size=2048
recdim=64
#the learning rate
lr=.001
#the weight decay for l2 normalizaton
weight_decay=1e-4
#the batch size for bpr loss training procedure
testbatch=128
#available datasets: {lastfm, gowalla, yelp2018, amazon-book,movielens}
dataset=movielens
#path to save weights
path=./saved_models
#@k test list
topks=[10]
num_epoch=1000
#whether we use multiprocessing or not in test
multicore=0
#whether we use pretrained weight or not
pretrain=0
#random
seed=2022
#whether we load test data of movielens or not
is_load_test_dataset=1
#verbose mode, show log in console
verbose=1
#epoch % [test_step] == 0: start testing
test_step=10
#weight initializator, support:{normal, xavier}
init=xavier
#optimization algorithms, support:{adam, adagrad}
optim=adam
#save models or not, default not
save_flag=True
# temperature
temp=0.2

# adj_type=plain, norm, gcmc, pre
adj_type=pre

# logits ['cosin', 'inner_product']
logits='cosin'