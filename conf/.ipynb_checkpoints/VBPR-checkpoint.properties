[hyperparameters]
reg=1e-4
description="Go MF"
#the batch size for bpr loss training procedure
batch_size=2048
#the embedding size of lightGCN
recdim=64
# loss function
loss="bpr_loss"
#the learning rate
lr=.001
#the weight decay for l2 normalizaton
weight_decay=1e-4
#the batch size for bpr loss training procedure
testbatch=128
#available datasets: {lastfm, gowalla, yelp2018, amazon-book,movielens}
dataset=movielens
#path to save weights
path=./checkpoints
#@k test list
topks=[10]
num_epoch=1000
#whether we use multiprocessing or not in test
multicore=0
#whether we use pretrained weight or not
pretrain=0
#random
seed=2022
#verbose mode, show log in console
verbose=1
#epoch % [test_step] == 0: start testing
test_step=10
#weight initializator, support:{normal, xavier}
init=xavier
#optimization algorithms, support:{adam, adagrad}
optim=adam
#log to server
log_api=
#save models or not, default not
save_flag=True
#the fold num used to split large adj matrix, like gowalla
a_fold=100

train_type=normal
