[hyperparameters]
layer_num=3
reg=1e-4
ssl_mode=both
description="Go MMGCN"
#the batch size for bpr loss training procedure
batch_size=2048
#the embedding size of lightGCN
recdim=64
# aggr mode
aggr_mode="add"
concat=False
# fastloss alpha
alpha=20
#the learning rate
lr=.001
#the weight decay for l2 normalizaton
weight_decay=1e-4
#using the dropout or not
dropout=0
#the batch size for bpr loss training procedure
keepprob=.6
#the fold num used to split large adj matrix, like gowalla
a_fold=100
#the batch size for bpr loss training procedure
testbatch=128
#available datasets: {lastfm, gowalla, yelp2018, amazon-book,movielens}
dataset=movielens
#path to save weights
path=./checkpoints
#@k test list
topks=[10]
num_epoch=1000
#whether we use multiprocessing or not in test
multicore=0
#whether we use pretrained weight or not
pretrain=0
#random
seed=2022
#verbose mode, show log in console
verbose=1
#epoch % [test_step] == 0: start testing
test_step=10
#weight initializator, support:{normal, xavier}
init=xavier
#optimization algorithms, support:{adam, adagrad}
optim=adam
#save models or not, default not
save_flag=True
log_api=
#other
A_split=False

pre_train=False
